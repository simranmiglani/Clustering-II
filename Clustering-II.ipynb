{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "MSIS522_Lab4_clustering_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL_xTajZWzsI"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.\n",
        "\n",
        "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in sklearn.cluster.KMeans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap2sproHWzsJ"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dd0glQkWzsN"
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CF7ateEWzsQ"
      },
      "source": [
        "The *k*-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.\n",
        "It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
        "\n",
        "- The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
        "- Each point is closer to its own cluster center than to other cluster centers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1jFkWtRWzsR"
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=1)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwXOeQO_WzsU"
      },
      "source": [
        "By eye, it is relatively easy to pick out the four clusters.\n",
        "The *k*-means algorithm does this automatically, and in Scikit-Learn uses the typical estimator API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzLYO154WzsV"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "print(y_kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI6VhFjRWzsY"
      },
      "source": [
        "Let's visualize the results by plotting the data colored by these labels.\n",
        "We will also plot the cluster centers as determined by the *k*-means estimator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMSoLBO1WzsY"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50)\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5BLmkYVWzsb"
      },
      "source": [
        "The good news is that the k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly. Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to k-means involves an intuitive iterative approach known as expectation–maximization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK6zuy3uWzsc"
      },
      "source": [
        "### Hierarchical Clustering\n",
        "TODO: apply hierarchical clustering to cluster this dataset using [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) and then generate the scatter plot of the clustering results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGsAli1BWzsd"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "hc = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')  \n",
        "y_ward = hc.fit_predict(X)  \n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_ward, s=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm5jK2z8Wzsf"
      },
      "source": [
        "TODO: Generate the dendrogram using [dendrogram](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html) function and [linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) function for this dataset. Can you tell what's the right number of clusters from the dendrogram?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKZVNAgmWzsg"
      },
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(10, 7))  \n",
        "plt.title(\"Dendograms\")  \n",
        "dend = shc.dendrogram(shc.linkage(X, method='ward'))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yQOh3yuWzsi"
      },
      "source": [
        "## Clustering Digits\n",
        "\n",
        "Let's take a look at applying *k*-means on the same simple digits data.\n",
        "\n",
        "Here we will attempt to use *k*-means to try to identify similar digits *without using the original label information*; this might be similar to a first step in extracting meaning from a new dataset about which you don't have any *a priori* label information.\n",
        "\n",
        "We will start by loading the digits and then finding the ``KMeans`` clusters.\n",
        "Recall that the digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjM6XtRwWzsj"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqGsdTzpWzsl"
      },
      "source": [
        "**TODO: Perform K-means with the number of clusters of 10.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT3cFqnXWzsm"
      },
      "source": [
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "clusters = kmeans.fit_predict(digits.data)\n",
        "kmeans.cluster_centers_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E24MPc1_Wzso"
      },
      "source": [
        "The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the \"typical\" digit within the cluster. Let's see what these cluster centers look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUXJGDQBWzsp"
      },
      "source": [
        "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
        "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
        "for axi, center in zip(ax.flat, centers):\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0U6ZciMWzsr"
      },
      "source": [
        "We see that even without the labels, KMeans is able to find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8.\n",
        "\n",
        "Because k-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted. We can fix this by matching each learned cluster label with the true labels found in them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPqhWv1zWzss"
      },
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(10):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQfY4yUrWzsu"
      },
      "source": [
        "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqL2d8eYWzsv"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(digits.target, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efbFx8NlWzsy"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(digits.target, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=digits.target_names,\n",
        "            yticklabels=digits.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czC5BJKWzs0"
      },
      "source": [
        "As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones.\n",
        "But this still shows that using *k*-means, we can essentially build a digit classifier *without reference to any known labels*!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ_v44ONWzs1"
      },
      "source": [
        "## *k*-means for color compression\n",
        "\n",
        "One interesting application of clustering is in color compression within images.\n",
        "For example, imagine you have an image with millions of colors.\n",
        "In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGbwRIuLWzs1"
      },
      "source": [
        "# Note: this requires the ``pillow`` package to be installed\n",
        "from sklearn.datasets import load_sample_image\n",
        "china = load_sample_image(\"china.jpg\")\n",
        "print(china.shape)\n",
        "ax = plt.axes(xticks=[], yticks=[])\n",
        "ax.imshow(china)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZKKZQP_Wzs4"
      },
      "source": [
        "data = china / 255.0 # use 0...1 scale\n",
        "data = data.reshape(427 * 640, 3)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxJTb4c8Wzs6"
      },
      "source": [
        "def plot_pixels(data, title, colors=None, N=10000):\n",
        "    if colors is None:\n",
        "        colors = data\n",
        "    \n",
        "    # choose a random subset\n",
        "    rng = np.random.RandomState(0)\n",
        "    i = rng.permutation(data.shape[0])[:N]\n",
        "    colors = colors[i]\n",
        "    R, G, B = data[i].T\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    ax[0].scatter(R, G, color=colors, marker='.')\n",
        "    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n",
        "\n",
        "    ax[1].scatter(R, B, color=colors, marker='.')\n",
        "    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n",
        "\n",
        "    fig.suptitle(title, size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "keFEEuAQWztC"
      },
      "source": [
        "plot_pixels(data, title='Input color space: 16 million possible colors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H5IbNo9WztG"
      },
      "source": [
        "Now let's reduce these 16 million colors to just 16 colors, using a k-means clustering across the pixel space. Because we are dealing with a very large dataset, we will use the mini batch k-means, which operates on subsets of the data to compute the result much more quickly than the standard k-means algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3w3OaNgtWztI"
      },
      "source": [
        "import warnings; warnings.simplefilter('ignore')  # Fix NumPy issues.\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "kmeans = MiniBatchKMeans(16)\n",
        "kmeans.fit(data)\n",
        "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n",
        "\n",
        "plot_pixels(data, colors=new_colors,\n",
        "            title=\"Reduced color space: 16 colors\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9CI6ouYWztM"
      },
      "source": [
        "The result is a re-coloring of the original pixels, where each pixel is assigned the color of its closest cluster center. Plotting these new colors in the image space rather than the pixel space shows us the effect of this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un8qjyBlWztN"
      },
      "source": [
        "china_recolored = new_colors.reshape(china.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
        "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].imshow(china)\n",
        "ax[0].set_title('Original Image', size=16)\n",
        "ax[1].imshow(china_recolored)\n",
        "ax[1].set_title('16-color Image', size=16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d05D29PBWztQ"
      },
      "source": [
        "Some detail is certainly lost in the rightmost panel, but the overall image is still easily recognizable.\n",
        "This image on the right achieves a compression factor of around 1 million!\n",
        "While this is an interesting application of *k*-means, there are certainly better way to compress information in images.\n",
        "But the example shows the power of thinking outside of the box with unsupervised methods like *k*-means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9hMGg13WztR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}